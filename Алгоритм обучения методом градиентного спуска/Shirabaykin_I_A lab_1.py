# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p6g8zZHjpc-6gf5cq2y7Y3yh7zT7D5fG
"""

import numpy as np

"""Шаг 1. Подготовить обучающую выборку"""

def prepare_training_set(q, n, k):
    X = np.random.rand(q, n)  # Обучающие векторы
    D = np.random.randint(0, 2, (q, k))  # Векторы желаемых значений выходов персептрона
    return X, D

"""Шаг 2. Генератором случайных чисел всем синаптическим весам и нейронным смещениям присваиваются некоторые малые случайные значения

"""

def initialize_weights(n, k):
    w = np.random.uniform(-0.5, 0.5, (n + 1, k))  # Синаптические веса и нейронные смещения
    return w

"""Шаг 3. Общей ошибке Eобщ и всем значениям коррекции синаптических весов присвоить нулевое значение

"""

def initialize_error_and_weight_correction(k):
    E_total = 0
    delta_w = np.zeros((n + 1, k))
    return E_total, delta_w

"""Шаг 4. Из обучающей выборки взять следующий по счету вектор Xm и подать его на входы персептрона

"""

def get_next_input_vector(X, m):
    x = X[m]
    x = np.insert(x, 0, 1)  # Добавляем смещение x0 = 1
    return x

"""Шаг 5. Для каждого j-го нейрона вычислить взвешенную сумму входных сигналов netj и выходной сигнал yj на основании функции активации f

"""

def calculate_output(x, w):
    net = np.dot(x, w)
    y = 1 / (1 + np.exp(-net))  # Функция активации сигмоид
    return y

"""Шаг 6. Вычислить ошибку E для текущего обучающего вектора и сложить полученное значение с общей ошибкой Eобщ

"""

def calculate_error(y, d):
    error = d - y
    E = np.sum(error ** 2)
    return E, error

"""Шаг 7. Посчитать величину коррекции синаптических весов j-го нейрона и нейронных смещений и запомнить их (накопление изменений)

"""

def accumulate_weight_correction(x, error, delta_w):
    delta_w += np.outer(x, error)
    return delta_w

"""Пример использования алгоритма

"""

q = 10  # Количество обучающих векторов
n = 3  # Количество входов
k = 2  # Количество нейронов
learning_rate = 0.1  # Скорость обучения
epochs = 100  # Количество эпох
threshold = 0.01  # Порог ошибки для останова обучения

X, D = prepare_training_set(q, n, k)
w = initialize_weights(n, k)

for epoch in range(epochs):
    E_total, delta_w = initialize_error_and_weight_correction(k)

    for m in range(q):
        x = get_next_input_vector(X, m)
        y = calculate_output(x, w)

        # Шаг 6
        E, error = calculate_error(y, D[m])
        E_total += E

        # Шаг 7
        delta_w = accumulate_weight_correction(x, error, delta_w)

    # Шаг 8
    if E_total < threshold:  # Критерий останова обучения по порогу ошибки
        print("Обучение завершено досрочно после", epoch + 1, "эпох")
        break

    # Дополнительный критерий останова: проверка изменения общей ошибки
    if epoch > 0 and abs(E_total - prev_E_total) < 0.001:
        print("Обучение завершено досрочно после", epoch + 1, "эпох (ошибка стабилизировалась)")
        break
    prev_E_total = E_total

    # Шаг 9
    w += learning_rate * delta_w

print("Окончательные веса:", w)
print("Общая ошибка:", E_total)

"""Теперь добавь реальное обучение keras данным алгоритмом"""

!pip install tensorflow
!pip install keras
import numpy as np
from tensorflow import keras
from keras.layers import Dense
from keras.models import Sequential, load_model

# Шаг 1. Подготовить обучающую выборку
def prepare_training_set(q, n, k):
    X = np.random.rand(q, n)  # Обучающие векторы
    D = np.random.randint(0, 2, (q, k))  # Векторы желаемых значений выходов персептрона
    return X, D

# Пример использования алгоритма с Keras
q = 100  # Количество обучающих векторов
n = 3  # Количество входов
k = 2  # Количество нейронов
epochs = 100  # Количество эпох
threshold = 0.1  # Порог ошибки для останова обучения

X, D = prepare_training_set(q, n, k)

# Создание модели персептрона с использованием Keras
model = Sequential()
model.add(Dense(k, input_dim=n, activation='sigmoid'))

# Компиляция модели
model.compile(loss='mean_squared_error', optimizer='sgd')

# Обучение модели
history = model.fit(X, D, epochs=epochs, verbose=0)

# Определение причины останова обучения
final_epoch = len(history.history['loss'])
if history.history['loss'][-1] < threshold:
    stop_reason = "Обучение завершено досрочно, так как ошибка стала меньше порогового значения."
else:
    stop_reason = "Обучение завершено после достижения максимального количества эпох."

# Тестирование модели и вывод результатов
predictions = model.predict(X)
print("Predictions:")
print(predictions)
print(stop_reason)